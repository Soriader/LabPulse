# LabPulse

![Tests](https://github.com/Soriader/LabPulse/actions/workflows/tests.yml/badge.svg)

LabPulse is a modular data processing framework for laboratory fuel quality analysis.

It implements a reproducible data pipeline with cleaning, validation rules, outlier detection,
REST API exposure, interactive dashboard visualization, and continuous integration testing.

This project demonstrates practical data engineering concepts:
modular architecture, versioned data outputs, automated testing, API design,
and dashboard-based data presentation.

---

## ğŸš€ Features

- Data cleaning and validation
- Unit consistency checks (QC rules)
- IQR-based outlier detection
- Versioned processed outputs (timestamped runs)
- REST API (FastAPI)
- Interactive dashboard (Streamlit)
- Automated unit tests (pytest)
- Continuous Integration (GitHub Actions)

---

## ğŸ— Architecture

```
Raw data (data/raw)
        â†“
Pipeline (cleaning â†’ QC â†’ outliers)
        â†“
Versioned outputs (data/processed/<run_id>/)
        â†“
FastAPI (JSON endpoints)
        â†“
Streamlit Dashboard (interactive UI)
```

Each pipeline execution generates a timestamped directory,
ensuring reproducibility and historical traceability.

The API always reads from the latest processed run.

---

## ğŸ“¥ Expected Input Format

Raw CSV files placed in `data/raw/` must contain the following columns:

- `sample_id` (int)
- `product` (str)
- `parameter` (str)
- `value` (numeric or string)
- `unit` (str)
- `date` (YYYY-MM-DD)

Example:

```
sample_id,product,parameter,value,unit,date
44,JetA1,Viscosity,137.2,cSt,2026-01-02
101,HSFO,Ash,0.0136,% m/m,2026-01-07
```

---

## ğŸ“¦ Installation

Clone the repository and install in editable mode:

```bash
pip install -e .
```

Optional dependencies:

```bash
pip install .[dev]        # pytest
pip install .[dashboard]  # streamlit + requests
```

---

## âš™ï¸ Running the Pipeline

To process raw data and generate a new versioned run:

```bash
python main.py
```

This creates a new folder:

```
data/processed/<YYYY-MM-DD_HH-MM-SS>/
```

Containing:

- `samples_cleaned.csv`
- `alerts_outliers_iqr.csv`

Each execution preserves previous results.

---

## ğŸŒ Running the API

Start the FastAPI server:

```bash
uvicorn labpulse.api.main:app --reload
```

Available endpoints:

- `GET /health`
- `GET /runs/latest`
- `GET /samples/latest`
- `GET /alerts/latest`

Interactive API documentation:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ“Š Running the Dashboard

In a separate terminal:

```bash
streamlit run dashboard.py
```

Dashboard runs at:

```
http://localhost:8501
```

The dashboard consumes data from the FastAPI backend.

Make sure the API server is running before launching the dashboard.

---

## ğŸ§ª Running Tests

```bash
python -m pytest
```

Tests cover:

- Cleaning logic
- QC rule validation
- Outlier detection
- Pipeline execution
- Data persistence
- API endpoints

CI is automatically executed on every push via GitHub Actions.

---

## ğŸ“ Project Structure

```
LabPulse/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_load_and_audit/
â”‚   â”œâ”€â”€ 02_cleaning/
â”‚   â””â”€â”€ 03_analysis/
â”‚
â”œâ”€â”€ src/labpulse/
â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”œâ”€â”€ qc_rules.py
â”‚   â”œâ”€â”€ alerts.py
â”‚   â”œâ”€â”€ io_utils.py
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ main.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ”® Future Improvements

- Pagination and filtering in API endpoints
- Authentication for API access
- Advanced statistical quality metrics
- Dockerized deployment
- Cloud deployment (AWS/Azure/GCP)
- Role-based access control
- Data quality scoring system

---

## ğŸ“„ License

MIT License